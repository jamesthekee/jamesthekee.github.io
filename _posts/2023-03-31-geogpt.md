---
layout: post
title:  "Quizzing ChatGPT on geography"
date:  2023-03-31 12:00:00 +0000
tags: geography chatgpt stats
published: false
---

# Geography NLP

*This is an experimental type of post and so may be lacking. I have tried to edit this down to a minimum but you shouldn’t expect high quality.*

**Summary:** I investigated ChatGPT’s ability to recall the coordinates of a city, using a script to test it on about 800 cities using the OpenAI API. I find the results didn’t support my expectations that the reliability of ChatGPT's coordinate recall didn’t solely correlate with population , tourism or another measure of prominence. I still believe my expectation is loosely correct, with some exceptions and deviations.

## Where this began

ChatGPT has demonstrated very impressive capabilities, so one day I wanted to test some less consequential abilities of it. I ended up testing it’s geographical knowledge and from a few examples, I found it could:

1. Accurately recall coordinates of a given city
2. Calculate the great-circle distance between any two cities
3. Calculate bearings between two cities
4. Fairly accurately fetch cities nearby a coordinate
5. Fairly accurately fetch capital cities nearby a coordinate
6. Figure out the country a coordinate was in

Rather surprisingly I also discovered it **couldn’t** calculate the distance or bearings between two coordinates. 

I thought this was interesting ground to investigate. Especially if ChatGPT grokked how to calculate great-circle distances. I decided to begin by looking at the simplest task, recalling coordinates. This is entire post is dedicated to that, I may investigate the others later but most likely not. 

This is a rather boring task to look at by itself. But my other reason for it was that when I saw ChatGPT did any of these tasks I thought I observed variable amounts of error, and it seemed to be greater for areas I considered obscure. Perhaps investigating the simplest task will also give a measurement of what cities ChatGPT considers obscure.

*An example of how ChatGPT performed on coordinate recall from manual input:*

| City | Actual coordinates | Predicted |
| --- | --- | --- |
| Kolkata, India | 22.5726° N, 88.3639° E. | 22.5726, 88.3639 |
| Kaduna, Nigeria | 10.5264° N, 7.4388° E. | 10.5319° N, 7.4294° E |
| Shiyan, China | 32.6292° N, 110.7980° E. | 22.8389° N, 120.4646° E |

## My Approach

I used this cities database from [kaggle](https://www.kaggle.com/datasets/juanmah/world-cities), giving over 28,000 cities with their coordinates, population and other data. I filtered the cities down by population to include only those of populations of over 200,000, leaving us with 2,800 cities. I created a function that queries the OpenAI chat API using the GPT-3.5-turbo model using the prompt below; and without change to any model parameters. (Therefore it using the default temperature of 1).

> **System**: “You are a helpful assistant, give the coordinates of a city, when provided. Answer tersely, to 6 decimal places, in the format X°(N/S), Y°(E/W). “
**User**:”{city}”
> 

This prompt was designed to:

1. Reduce the total amount of tokens of the response, 
2. Get the coordinates in a *mostly* universal format rather than the possibility of -50.5°, 12.5° or 50.5°W, 12.5°N.
3. Coax ChatGPT to give a precise answer as possible

I handled the case of ChatGPT needing disambiguation (like below), by providing it also with the country. One might see issues with this but I’ll discuss that here[1].

> Sorry, could you please provide more information on which Concord you are referring to? There are multiple cities/towns/villages with this name in different parts of the world.”
> 

Once I had it working I got it running on my database, I did some rather unsystematic sampling[2] that I don’t have the patience to completely iron out, though it remains useful enough. This sample contains around 800 cities, from all continents and is slightly skewed towards very populous nations (e.g. China).

### Calculating error

Once I obtained the coordinates from GPT I calculated the error. For the error, I decided to use the great-circle distance between the actual and provided distances, using kilometres as the units. Since this is using the haversine formula, I call this value haversine throughout the rest of this post.

## Findings

Before reading about the findings, consider taking this time to make some predictions. Will ,or will there not be a correlation of beginning I expected  

The first graph I plotted was haversine distance against the population of the city.

<img class="img1" src="/assets/images/geogpt1/gw_chart_336034.png"/>

This showed many guesses that were completely off. For reference the greatest distance possible, which is half the circumference of the earth, is around 20,000km. I examined every data point with a haversine distance of over 1,000km and found nearly all where from ambiguous cases or some strange oddity, more detail here[5]. I removed these anomalies from the data and then got something that looks like this.

<img class="img1" src="/assets/images/geogpt1/gw_chart_293117.png"/>

There’s a lot of overlap, so let’s look at the log of haversine. I set the the minimum distance to 0.1km to avoid the issue of taking the log of 0, and colour each dot according to the continent it belongs to.

<img class="img1" src="/assets/images/geogpt1/gw_chart_105548.png"/>

This isn’t ideal, but it does demonstrate a few patterns. Those dots on the right, which are the cities above 30,000,000 million population (they correspond to Delhi, Jakarta, Tokyo from left to right) have a loghaversine that looks rather average between 0.5 and 1, which corresponds to a haversine of between 3km and 10km. If you google the coordinates of Tokyo you can find multiple different answers from many different sources, so it is possible this discord is the explanation for why it is so high for Tokyo. A similar effect could be affecting cities of similar scale and sprawl.

Almost every data point at that bottom line of loghaversine=-1 are prominent cities, though we can find many others not on this line. At this point I would suggest that this loghaversine error, depends on numerous factors beyond just the vague notion of popularity/prominence. Let’s just look at the other graphs.

Here’s a clear look at the overall distribution of the loghaversine. Because I used a logarithm on the haversine, bare in mind that the bins are not actually equally sized. 

<img class="img1" src="/assets/images/geogpt1/overall.png"/>

Oh and let’s also plot if they are capital cities or not. I can’t really see any significant difference that isn’t attributable to the smaller number of capitals in the sample.

<img class="img1" src="/assets/images/geogpt1/capitals.png"/>

Below are the plots grouped by each city’s continent, and the averages recorded into this table below. The order of the most accurate continents overall are quite surprising, especially North America being the highest. Though since this data is so complex it’s hard to even comment why any the order is like that.

<img class="img1" src="/assets/images/geogpt1/output3.png"/>

| Mean of | Africa | Asia | Europe | North America | South America | Oceania |
| --- | --- | --- | --- | --- | --- | --- |
| haversine | 12.0121 | 13.7024 | 1.0218 | 12.6534 | 0.5034 | 6.6091 |
| loghaversine | 0.073857 | 0.136755 | -0.241474 | 0.406831 | -0.440521 | -0.103868 |

Now here’s the average haversine distance, plotted against population. And because there isn’t enough unclear graphs, I made it into two similar histograms with different sizes bins. Which both give a sense that the simple negative correlation I was expecting, is simply not there. There is probably a complex correlation with like an S curve, but I think that would require too much work to demonstrate.

<img class="img1" src="/assets/images/geogpt1/output6.png"/>

<img class="img1" src="/assets/images/geogpt1/output5.png"/>

The histograms for different continents it looks like this. 

<img class="img1" src="/assets/images/geogpt1/combined.png"/>

A geographical plot looks like this.

<img class="img1" src="/assets/images/geogpt1/curious_project.jpg"/>

And if we separate every data point into 5 different groups G-1, G0, … to G3, each Gi containing data points with a loghaversine between i and i+1.

<img style="width: 1400px;" src="/assets/images/geogpt1/gw_chart_1118.png"/>

Another correlation I speculated was that loghaversine might correlate with tourism. Using this wikipedia [resource](https://en.wikipedia.org/wiki/List_of_cities_by_international_visitors), and mixing various tourist numbers for about 100 cities, plotting tourist numbers in (2018 or 2016 if they are missing) against log-haversine I get this. There is no visible correlation. Well I guess next project I’ll spend my time investigating something useful.

<img class="img1" src="/assets/images/geogpt1/Untitled.png"/>

## Conclusions

The resultant data is a bit too complex to find clear correlations and conclusive conclusions. The most interesting observations are that North America has such a high average haversine, and South America doing the lowest. I suggest that ChatGPT heavily memorises from is Wikipedia[2], and this is why it performs broadly well but this may cause some error since not all sources agree.

### Footnotes

[1] I didn’t want it to be given the country of the city, because that feels like your priming the model and not properly testing it.

[2] I only sample by doing a city once, I didn’t want to have to do it multiple times- though that would be more robust against random mistakes and then could measure the std. I did sampling by starting doing the top 376 populous cities (that didn’t fail during the request). This took me to the 523th sample, meaning 147 somehow failed. Then since I had so much China sampled I blacklisted it, then starting from the 1000th most populous samples, got 484 samples by incrementing by either 4 or 5. So yeah kind of scuffed when you want a representative sample. A graph below sort of shows how I sampled.

<img class="img1" src="/assets/images/geogpt1/sampling.png"/>

[3] Out of the 31, 28 turned out to be ambiguous city names, mostly from ones that I assume were American (North and south) colonial settlements. Examples like York, London, Córdoba. Amusing examples were ChatGPT confusing La Florida, Chile for the entire state of Florida and Babylon, New York for the ancient city of Babylon. There were a few oddities with guessing Ganda, Angola as a location in the Atlantic sea and Bắc Ninh, Vietnam in the Indian ocean. And the final one being a city called Nangandao, that ChatGPT thought was real and so did the database but I can’t find a location for it.

[4]  It seems plausible when you notice how well ChatGPT is at recalling information on obscure content that happens to be on Wikipedia. Observations like asking it about Babylon gives the coordinates of the ancient city rather than the town in New York. It can also easily recall well geolocational data of small English towns using the OS grid reference. It also memorises postal codes and populations fairly accurately. 

## Addendum

### API usage

Overall I spent a total of $0.21 on API calls, from 1646 total text completion calls. Since I obtained 857 records, that means half of all calls resulted in a timeout or required a disambiguation. Using the pricing of $0.002/1k tokens means I must have used 105,000 tokens total and about 60 tokens a request. 

### What I learnt

- Familiarity with OpenAI API. I am more aware that if I think about it logically, the cost of one prompt is minimal, so don’t be afraid to test anything. When using API’s with real life cost it’s best that you supress the programmer desire for efficiency when it is just holding back time. There’s no world where you should spend more than 10 minutes to just save a few cents.
- Python has no inbuilt library to add a timeout for a function for windows, and only exists for linux.
- Pygwalker is kind of a cool library for exploring data, though it there seems like some ridiculous features that should be in by default. Like you can’t have a scatter plot with constant opacity for each data point without creating a column in your data frame with just a constant value to put into the graph’s opacity attribute.

### Suggested side-quests

- How does it compare to other models? (GPTNeoX)
- Can we fabricate city names, and get ChatGPT to answer? My few tries seem to be no, but maybe a good query could coax an answer out. I think China might be the best bet as ChatGPT gave answers for cities I could find little evidence for.
- China, Japan and maybe several other countries have their city names not represented in their native language, this could bias the data in some way. Perhaps if this is repeated this issue should be properly considered.

[Distances](/2023/03/31/geogpt2.html)